{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from pycaret.classification import *\n",
    "\n",
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('spacy_pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Loaded\n"
     ]
    }
   ],
   "source": [
    "model = load_model('lightgbm_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(doc):\n",
    "    pos_count = {}\n",
    "    ent_count = {}\n",
    "    \n",
    "    pos = [token.tag_ for token in doc]\n",
    "    ents = [token.ent_type_ for token in doc]\n",
    "    \n",
    "    for tag in pos:\n",
    "        if tag in pos_count:\n",
    "            pos_count[tag] += 1\n",
    "        else:\n",
    "            pos_count[tag] = 1\n",
    "    \n",
    "    for tag in ents:\n",
    "        if tag in ent_count:\n",
    "            ent_count[tag] += 1\n",
    "        else:\n",
    "            ent_count[tag] = 1\n",
    "    \n",
    "    return pos_count, ent_count\n",
    "\n",
    "def get_embedding(doc):\n",
    "    tensor = doc._.trf_data.tensors[0]  \n",
    "    contextual_embedding = tensor[0, 0, :]     \n",
    "    return contextual_embedding\n",
    "\n",
    "def get_features(doc):\n",
    "    pos_tags = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"PRON\"]\n",
    "    ent_tags = [\"PER\", \"ORG\", \"LOC\"]\n",
    "\n",
    "    pos, ent = get_count(doc)\n",
    "    pos_count = [pos.get(tag, 0) for tag in pos_tags]\n",
    "    ent_count = [ent.get(tag, 0) for tag in ent_tags]\n",
    "    \n",
    "    token_count = len(doc)\n",
    "\n",
    "    contextual_embedding = get_embedding(doc)\n",
    "\n",
    "    features = {f'{tag}': count for tag, count in zip(pos_tags, pos_count)}\n",
    "    features.update({f'{tag}': count for tag, count in zip(ent_tags, ent_count)})\n",
    "    features['token_count'] = token_count\n",
    "    \n",
    "    for i, embedding in enumerate(contextual_embedding):\n",
    "        if hasattr(embedding, 'get'):\n",
    "            embedding = embedding.get()\n",
    "        \n",
    "        features[f'emb_{i}'] = embedding\n",
    "\n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('WELFake_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news = news[news['label'] == 0]\n",
    "fake_news = news[news['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_articles = real_news['text']\n",
    "fake_articles = fake_news['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_features = []\n",
    "for doc in nlp.pipe(real_articles, batch_size=64):\n",
    "    features = get_features(doc)\n",
    "    real_features.append(features)\n",
    "\n",
    "real_df = pd.DataFrame(real_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_articles = [article for article in fake_articles if isinstance(article, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_features = []\n",
    "for doc in nlp.pipe(fake_articles, batch_size=64):\n",
    "    features = get_features(doc)\n",
    "    fake_features.append(features)\n",
    "\n",
    "fake_df = pd.DataFrame(fake_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df.to_csv('WELFake_real.csv', index=False)\n",
    "fake_df.to_csv('WELFake_fake.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
