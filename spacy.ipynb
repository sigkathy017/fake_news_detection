{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "pos_dataset = load_dataset(\"universal_dependencies\", \"en_ewt\", split=\"train\")\n",
    "ner_dataset = load_dataset(\"conll2003\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = pos_dataset.features[\"upos\"].feature.names\n",
    "ner_labels = ner_dataset.features[\"ner_tags\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.ner.EntityRecognizer at 0x1bb06699690>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\n",
    "    \"transformer\",\n",
    "    config={\n",
    "        \"model\": {\n",
    "            \"@architectures\": \"spacy-transformers.TransformerModel.v3\",\n",
    "            \"name\": \"tvocoder/bert_fake_news_ft\",\n",
    "            \"tokenizer_config\": {\n",
    "                \"use_fast\": True,\n",
    "                \"do_lower_case\": True,\n",
    "                \"truncation\": True,\n",
    "                \"max_length\": 512,\n",
    "                \"padding\": \"max_length\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "nlp.add_pipe(\"tagger\")\n",
    "nlp.add_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example\n",
    "\n",
    "from spacy.training.iob_utils import iob_to_biluo, biluo_tags_to_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_training_examples(pos_dataset):\n",
    "    labels = pos_dataset.features[\"upos\"].feature.names\n",
    "    examples = []\n",
    "    for example in pos_dataset:\n",
    "        doc = Doc(nlp.vocab, words=example[\"tokens\"])\n",
    "        pos = [labels[i] for i in example[\"upos\"]]\n",
    "        examples.append(Example.from_dict(doc, {\"tags\": pos}))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_training_examples(ner_dataset):\n",
    "    labels = ner_dataset.features[\"ner_tags\"].feature.names\n",
    "    examples = []\n",
    "    for example in ner_dataset:\n",
    "        doc  = Doc(nlp.vocab, words=example[\"tokens\"])\n",
    "        iob  = [labels[i] for i in example[\"ner_tags\"]]\n",
    "        biluo= iob_to_biluo(iob)\n",
    "        ents = biluo_tags_to_offsets(doc, biluo)\n",
    "        examples.append(Example.from_dict(doc, {\"entities\": ents}))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN', 'PUNCT', 'ADP', 'NUM', 'SYM', 'SCONJ', 'ADJ', 'PART', 'DET', 'CCONJ', 'PROPN', 'PRON', 'X', '_', 'ADV', 'INTJ', 'VERB', 'AUX']\n"
     ]
    }
   ],
   "source": [
    "pos_labels = pos_dataset.features[\"upos\"].feature.names\n",
    "print(list(pos_labels))\n",
    "\n",
    "for label in pos_labels:\n",
    "    nlp.get_pipe(\"tagger\").add_label(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    }
   ],
   "source": [
    "ner_labels = ner_dataset.features[\"ner_tags\"].feature.names\n",
    "print(list(ner_labels))\n",
    "\n",
    "for label in ner_labels:\n",
    "    nlp.get_pipe(\"ner\").add_label(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = pos_training_examples(pos_dataset)\n",
    "ner_train = ner_training_examples(ner_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from spacy.util import minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "nlp.initialize(lambda: pos_train)\n",
    "nlp.initialize(lambda: ner_train)\n",
    "\n",
    "optimizer = nlp.create_optimizer()\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses = {}\n",
    "    random.shuffle(pos_train + ner_train)\n",
    "    \n",
    "    batches = minibatch(pos_train + ner_train, size=batch_size)\n",
    "    for batch in batches:\n",
    "        nlp.update(batch, sgd=optimizer, drop=0.5, losses=losses)\n",
    "    print(f\"[Epoch {epoch+1}] Losses: {losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"./spacy_pipeline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
